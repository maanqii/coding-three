{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maanqii/coding-three/blob/main/ml/CartoonGAN_TFLite_Fixed_Shaped.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ob9My2EnpBD"
      },
      "source": [
        "This notebook demonstrates the TFLite model conversion process for the **White-box Cartoonization** model as proposed in the paper [Learning to Cartoonize Using White-box Cartoon Representations](https://github.com/SystemErrorWang/White-box-Cartoonization/blob/master/paper/06791.pdf). Below you can find an example output that this model is capable of producing:\n",
        "\n",
        "![](https://i.ibb.co/DkzL9J7/Screen-Shot-2020-07-21-at-1-42-25-PM.png)\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[GitHub repository of the paper](https://github.com/SystemErrorWang/White-box-Cartoonization)\n",
        "\n",
        "\n",
        "Code for the paper is available here in [this repository](https://github.com/SystemErrorWang/White-box-Cartoonization/).\n",
        "\n",
        "\n",
        "**Acknowledgements**: Khanh LeViet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzMpRR-xfDhg"
      },
      "source": [
        "## Setup and imports\n",
        "\n",
        "As the main model is implemented in TensorFlow 1 it's important for us replicate a TensorFlow 1.x runtime to be able to load the models weights, perform inference, and export a `SavedModel`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIYdn1woOS1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030e7f00-ef0f-45c4-e657-117ec645bde1"
      },
      "source": [
        "!git clone https://github.com/SystemErrorWang/White-box-Cartoonization"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'White-box-Cartoonization' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZFinypsgbjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "bf16ba21-6f92-432f-987f-7a3a1d6df781"
      },
      "source": [
        "import sys\n",
        "sys.path.append('./White-box-Cartoonization/test_code')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cartoonize\n",
        "import os\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3c2194f4d9e7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcartoonize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/./White-box-Cartoonization/test_code/cartoonize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mguided_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/./White-box-Cartoonization/test_code/network.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVtrZeUhhPaF"
      },
      "source": [
        "## Gather sample data and run inference to make sure the model works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsiEgsWQh6Df"
      },
      "source": [
        "!mkdir ./source-frames\n",
        "!wget -O image.jpg https://raw.githubusercontent.com/sayakpaul/portfolio/master/images/image_sayak.jpg\n",
        "!mv image.jpg ./source-frames/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RE_1OEBirui"
      },
      "source": [
        "model_path = './White-box-Cartoonization/test_code/saved_models'\n",
        "load_folder = './source-frames'\n",
        "save_folder = './cartoonized_images'\n",
        "if not os.path.exists(save_folder):\n",
        "    os.mkdir(save_folder)\n",
        "\n",
        "cartoonize.cartoonize(load_folder, save_folder, model_path)\n",
        "\n",
        "source_image = plt.imread('./source-frames/image.jpg')\n",
        "cartoonized_image = plt.imread('./cartoonized_images/image.jpg')\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(source_image)\n",
        "plt.title('Source image')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cartoonized_image)\n",
        "plt.title('Cartoonized image')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZA9y9CRjRV7"
      },
      "source": [
        "## Create a `SavedModel` from the pre-trained checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psLizL1VioNf"
      },
      "source": [
        "# Directory where SavedModel is to be saved\n",
        "!mkdir saved_model_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8GMfHQblTxo"
      },
      "source": [
        "# Comes with the GitHub repo\n",
        "# These will be needed to instantiate the model\n",
        "import network\n",
        "import guided_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDCYfnLTmtL-"
      },
      "source": [
        "# We will be using TensorFlow session\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "443euohvmwnz"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.Session(config=config) as sess:\n",
        "  # Create placeholder for the input\n",
        "  input_photo = tf.placeholder(tf.float32, [1, None, None, 3], name='input_photo')\n",
        "\n",
        "  # Run the input placeholder through the generator, and then apply a\n",
        "  # filter to process the generator output\n",
        "  network_out = network.unet_generator(input_photo)\n",
        "  final_out = guided_filter.guided_filter(input_photo, network_out, r=1, eps=5e-3)\n",
        "  final_out = tf.identity(final_out, name='final_output') # Create an identical filtering layer\n",
        "\n",
        "  # The above process is basically needed to construct the computation graph for the\n",
        "  # current session\n",
        "\n",
        "  # Get the generator variables and restore the pre-trained checkpoints in the\n",
        "  # current session\n",
        "  all_vars = tf.trainable_variables()\n",
        "  gene_vars = [var for var in all_vars if 'generator' in var.name]\n",
        "  saver = tf.train.Saver(var_list=gene_vars)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
        "\n",
        "  # Export to SavedModel\n",
        "  tf.saved_model.simple_save(\n",
        "      sess,\n",
        "      '/content/saved_model_dir',\n",
        "      inputs={input_photo.name: input_photo},\n",
        "      outputs={final_out.name: final_out})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB2eD172k37O"
      },
      "source": [
        "### Inspect the files sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC2YXF5-ntGS"
      },
      "source": [
        "!ls -lh saved_model_dir\n",
        "!ls -lh saved_model_dir/variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEDEGgo0n0Te"
      },
      "source": [
        "!ls -lh /content/White-box-Cartoonization/test_code/saved_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI0hNJigpZe4"
      },
      "source": [
        "**A runtime restart is required here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYXFM8h-k_DY"
      },
      "source": [
        "# TFLite model conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJzm5J42lhi4"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC-E9ydKmmpa"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTgYCywznIqM"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO-c9KfW8KjI"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQGzhRTyluW4"
      },
      "source": [
        "## Dynamic-range quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43QXD7RIngeB"
      },
      "source": [
        "model = tf.saved_model.load('saved_model_dir')\n",
        "concrete_func = model.signatures[\n",
        "    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "concrete_func.inputs[0].set_shape([1, 512, 512, 3]) # Just comment this line if you want to export a model with dynamic shape support\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open('whitebox_cartoon_gan_dr.tflite', 'wb').write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRHEnGzKpIDE"
      },
      "source": [
        "!ls -lh whitebox_cartoon_gan_dr.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyTwLZ7KmD3O"
      },
      "source": [
        "## float16 quantization\n",
        "\n",
        "We further reduce the size of the inputs for float16 model so that it runs reasonably faster on an Android device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD12P_tlmGAj"
      },
      "source": [
        "model = tf.saved_model.load('saved_model_dir')\n",
        "concrete_func = model.signatures[\n",
        "    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "concrete_func.inputs[0].set_shape([1, 224, 224, 3])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open('whitebox_cartoon_gan_fp16.tflite', 'wb').write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_sMTemmmWdD"
      },
      "source": [
        "!ls -lh whitebox_cartoon_gan_fp16.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucGU_e1rnL0s"
      },
      "source": [
        "## int8 quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpmTSJUZ7DUq"
      },
      "source": [
        "# Gather some more data\n",
        "!wget -O image2.jpg https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/belfry-2611573_1280.jpg\n",
        "!wget -O image3.jpg https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg\n",
        "!wget -O image4.jpg https://pbs.twimg.com/profile_images/1260205070555086855/erU_iILT_400x400.jpg\n",
        "!wget -O image5.jpg https://pbs.twimg.com/profile_images/1235595938921459713/h26CpAPb_400x400.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7OjYyoG70Na"
      },
      "source": [
        "# Put these images in a central directory\n",
        "!cp -r *.jpg source-frames/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01ohOx0ynLaq"
      },
      "source": [
        "IMG_SIZE = 512\n",
        "images_list = os.listdir('/content/source-frames')\n",
        "\n",
        "# int8 quantization requires a representative dataset generator\n",
        "def representative_dataset_gen():\n",
        "    for image_path in images_list:\n",
        "        image = cv2.imread(os.path.join('/content/source-frames', image_path))\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        image = image.astype(np.float32)/127.5 - 1\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        yield [image]\n",
        "\n",
        "model = tf.saved_model.load('saved_model_dir')\n",
        "concrete_func = model.signatures[\n",
        "    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "concrete_func.inputs[0].set_shape([1, IMG_SIZE, IMG_SIZE, 3])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "open('whitebox_cartoon_gan_int8.tflite', 'wb').write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17JOYaMHoeI3"
      },
      "source": [
        "!ls -lh whitebox_cartoon_gan_int8.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVzfEdXWr5lK"
      },
      "source": [
        "## Sample inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dENoCZjW8OYY"
      },
      "source": [
        "# Reference: https://www.tensorflow.org/lite/models/style_transfer/overview\n",
        "def load_img(path_to_img):\n",
        "  img = cv2.imread(path_to_img)\n",
        "  img = img.astype(np.float32) / 127.5 - 1\n",
        "  img = np.expand_dims(img, 0)\n",
        "  img = tf.convert_to_tensor(img)\n",
        "  return img\n",
        "\n",
        "# Function to pre-process by resizing an central cropping it.\n",
        "def preprocess_image(image, target_dim=224):\n",
        "  # Resize the image so that the shorter dimension becomes the target dim.\n",
        "  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
        "  short_dim = min(shape)\n",
        "  scale = target_dim / short_dim\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "\n",
        "  # Central crop the image.\n",
        "  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIVzt8eRrttG"
      },
      "source": [
        "model_type = \"int8\" #@param [\"dr\", \"int8\", \"float16\"]\n",
        "source_image = load_img('/content/source-frames/image.jpg')\n",
        "if model_type == \"float16\":\n",
        "    preprocessed_source_image = preprocess_image(source_image, target_dim=224)\n",
        "else:\n",
        "    preprocessed_source_image = preprocess_image(source_image, target_dim=512)\n",
        "preprocessed_source_image.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moSasqoY8YVO"
      },
      "source": [
        "#@title Run inference\n",
        "model_dict = {\n",
        "    \"dr\": \"/content/whitebox_cartoon_gan_dr.tflite\",\n",
        "    \"int8\": \"/content/whitebox_cartoon_gan_int8.tflite\",\n",
        "    \"float16\": \"/content/whitebox_cartoon_gan_fp16.tflite\"\n",
        "}\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=model_dict[model_type])\n",
        "input_details = interpreter.get_input_details()\n",
        "\n",
        "# Comment out the following when dealing with models with dynamic shaped inputs\n",
        "# if model_type == 'int8':\n",
        "#     interpreter.resize_tensor_input(156, [1, preprocessed_source_image.shape[1], preprocessed_source_image.shape[2], 3])\n",
        "# else:\n",
        "#     interpreter.resize_tensor_input(0, [1, preprocessed_source_image.shape[1], preprocessed_source_image.shape[2], 3])\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "interpreter.set_tensor(input_details[0]['index'], preprocessed_source_image)\n",
        "interpreter.invoke()\n",
        "\n",
        "raw_prediction = interpreter.tensor(\n",
        "    interpreter.get_output_details()[0]['index'])()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3GYU9kOsrgX"
      },
      "source": [
        "## Process the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkbqEf6F-lJ_"
      },
      "source": [
        "output = (np.squeeze(raw_prediction)+1.0)*127.5\n",
        "output = np.clip(output, 0, 255).astype(np.uint8)\n",
        "output = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oig7Ib-c--vt"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(plt.imread('/content/source-frames/image.jpg'))\n",
        "plt.title('Source image')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(output)\n",
        "plt.title('Cartoonized image')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}